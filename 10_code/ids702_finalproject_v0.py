# -*- coding: utf-8 -*-
"""IDS702_FinalProject_v0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_kBdEUwph6l0nzPigWbUpnQksCbt__0K
"""

# from google.colab import drive
# drive.mount('/content/drive/')

# Commented out IPython magic to ensure Python compatibility.
# % cd /content/drive/My Drive/Open Data

# %%
from IPython.core.interactiveshell import InteractiveShell

InteractiveShell.ast_node_interactivity = "all"

# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.model_selection import train_test_split
from statsmodels.stats.outliers_influence import variance_inflation_factor

import statsmodels.api as sm
from statsmodels.formula.api import ols
from statsmodels.tools.eval_measures import rmse
import statsmodels.stats.api as sms
from statsmodels.compat import lzip
from statsmodels.stats import diagnostic as diag
import time

from pandas.tseries.holiday import USFederalHolidayCalendar as calendar

import seaborn as sns

pd.set_option("display.max_rows", 100)
pd.set_option("display.max_columns", 100)
pd.set_option("float_format", "{:.2f}".format)

# %%
# start_time=time.time()
df = pd.read_csv(
    "C:\\Users\\deeks\\Documents\\MIDS\\IDS 702_Modeling and representation of data\\Final Project\\\Datasets\\Police_Department_Incident_Reports__2018_to_Present(2).csv"
)
# print('Duration: {:.2f} seconds'.format(time.time()-start_time))
# df.head()
df.shape

# %%
df.info()

# %%
(df.isna().sum() / len(df)).apply(lambda x: "{:.2%}".format(x))

# %%
# Check if multiple incident categories per incident
(df.groupby("Incident Number")["Incident Category"].count() > 1).sum()

# %%
# Keeping one incident category per incident
df.drop_duplicates(subset="Incident Number", keep="first", inplace=True)
df.shape

# %%
drop_col_list = [
    "Incident Datetime",
    "Analysis Neighborhood",
    "Report Datetime",
    "Row ID",
    "Incident ID",
    "CAD Number",
    "Report Type Code",
    "Report Type Description",
    "Filed Online",
    "Incident Code",
    "Incident Category",
    "Incident Subcategory",
    "Incident Description",
    # "Resolution",
    "Intersection",
    "CNN",
    # "Police District",
    "Supervisor District",
    "Latitude",
    "Longitude",
    "Point",
    "Neighborhoods",
    "ESNCAG - Boundary File",
    "Central Market/Tenderloin Boundary Polygon - Updated",
    "Civic Center Harm Reduction Project Boundary",
    "HSOC Zones as of 2018-06-05",
    "Invest In Neighborhoods (IIN) Areas",
    "Current Supervisor Districts",
    "Current Police Districts",
]

# Dropping unwanted columns
df.drop(columns=drop_col_list, inplace=True)
print("Post dropping columns, dataframe shape reduces to ", df.shape)

# %%

(df.isna().sum() / len(df)).apply(lambda x: "{:.2%}".format(x))

# %%
rename_dict = {
    "Incident Date": "Date",
    "Incident Time": "Time",
    "Incident Year": "Year",
    "Incident Number": "IncidntNum",
    "Incident Day of Week": "DayOfWeek",
    "Police District": "PoliceDistrict",
}
df.rename(columns=rename_dict, inplace=True)
df.head()

# %%
# Engineering some additional features
df["Date"] = pd.to_datetime(df["Date"], format="%Y/%m/%d")
df["Month"] = df["Date"].dt.month

mth_season = {
    12: "Winter",
    1: "Winter",
    2: "Winter",
    3: "Spring",
    4: "Spring",
    5: "Spring",
    6: "Summer",
    7: "Summer",
    8: "Summer",
    9: "Fall",
    10: "Fall",
    11: "Fall",
}
df["Season"] = df["Month"].map(mth_season)

df["DayOfMonth"] = df["Date"].dt.day
df["End_Start_Month"] = np.where(
    df["DayOfMonth"].isin([1, 2, 3, 4, 5, 6, 7, 25, 26, 27, 28, 29, 30, 31]),
    "EoM/SoM",
    "Not_EoM/SoM",
)

df["HourOfDay"] = pd.to_datetime(df["Time"], format="%H:%M").dt.hour

hours_of_day = pd.DataFrame({"hour": range(0, 24)})
bin = [0, 4, 8, 12, 16, 20, 23]
label = ["Late Night", "Early Morning", "Morning", "Noon", "Evening", "Night"]
df["TimeOfDay"] = pd.cut(df["HourOfDay"], bins=bin, labels=label, include_lowest=True)

df["BusinessHour"] = np.where((df["HourOfDay"] >= 8) & (df["HourOfDay"] < 18), 1, 0)

df["isWeekend"] = [1 if row in ("Saturday", "Sunday") else 0 for row in df["DayOfWeek"]]

cal = calendar()
holidays = cal.holidays(start=df["Date"].min(), end=df["Date"].max())
df["Holiday"] = np.where(df["Date"].dt.date.astype("datetime64").isin(holidays), 1, 0)


df["WeekOfYear"] = df["Date"].dt.isocalendar().week

print("Post adding features, dataframe shape becomes ", df.shape)

drop_col_list2 = ["Date", "Time", "DayOfWeek", "HourOfDay"]
df.drop(columns=drop_col_list2, inplace=True)

df.head()

# %%
print("Unique count of Police Districts", df["PoliceDistrict"].value_counts())

# %%
# drop police districts outside of SF
df = df.loc[~df["PoliceDistrict"].isin(["Out of SF"])].copy()
df.shape

# %%
# Defining response
df["Resolution_resp"] = 0
df.loc[
    df["Resolution"].isin(["Cite or Arrest Adult", "Exceptional Adult"]),
    "Resolution_resp",
] = 1
pd.crosstab(df["Resolution"], df["Resolution_resp"])

# %%
df.head()


# # Calculating average of hour at which incidents occur
# agg_hour = pd.DataFrame(
#     df_raw_aug.groupby(["Date", "Analysis Neighborhoods"]).agg({"HourOfDay": "mean"})
# ).reset_index()
# agg_hour.rename(columns={"HourOfDay": "Mean_HourOfDay"}, inplace=True)
# agg_hour = agg_hour.round({"Mean_HourOfDay": 0})
# df_agg = df_raw_aug.merge(agg_hour, how="left", on=["Date", "Analysis Neighborhoods"])

# # Aggregating here by number of incidents per day per neighborhood
# agg_inc = pd.DataFrame(
#     df_raw_aug.groupby(["Date", "Analysis Neighborhoods"]).agg({"IncidntNum": "count"})
# ).reset_index()
# agg_inc.rename(columns={"IncidntNum": "IncidntCount"}, inplace=True)

# df_agg2 = df_agg.merge(agg_inc, how="left", on=["Date", "Analysis Neighborhoods"])
# print("Dataset rolled up at date neighborhood level")
# df_agg2.head(3)

# plt.subplots(figsize=(10, 6))
# matrix = np.triu(df_agg2.corr())

# sns.heatmap(
#     df_agg2.corr(),
#     annot=True,
#     cmap="coolwarm",
#     fmt=".0%",
#     annot_kws={"size": 9.5},
#     mask=matrix,
# )

# %%
